{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CERN CMS Particle Tracking\n## Introduction\nIn this notebook I will try to develop a **PyTorch** based model to predict particle collision from the European organisation CERN's CMS data.","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries\nLets import some important libraries and pytorch.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom math import ceil\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset, Sampler\n\nimport line_profiler\n%load_ext line_profiler\n\nDATASET_PATH = '../input/trackml/'\n\nfrom tqdm import tqdm_notebook\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/trackml/\"))\nprint(os.listdir(DATASET_PATH))\nprefix='../input/trackml-particle-identification/'\nimport zipfile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-05T13:11:56.137110Z","iopub.execute_input":"2022-01-05T13:11:56.137567Z","iopub.status.idle":"2022-01-05T13:11:56.152897Z","shell.execute_reply.started":"2022-01-05T13:11:56.137507Z","shell.execute_reply":"2022-01-05T13:11:56.151872Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"The line_profiler extension is already loaded. To reload it, use:\n  %reload_ext line_profiler\n['trackml', 'trackml-particle-identification']\n['my_tracks_all.npy', 'my_event000001001.npy', 'my_model.h5', 'my_model_h.h5']\n['my_tracks_all.npy', 'my_event000001001.npy', 'my_model.h5', 'my_model_h.h5']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let us define some important helper functions like a timer to measure time while training, another function which returns the type of event at CMS from data.","metadata":{}},{"cell_type":"code","source":"from contextlib import contextmanager\nfrom timeit import default_timer\n\n@contextmanager\ndef elapsed_timer():\n    start = default_timer()\n    elapser = lambda: default_timer() - start\n    yield lambda: elapser()\n    end = default_timer()\n    elapser = lambda: end-start","metadata":{"_uuid":"c4d2d5e5dd576002fc170d7244005d5f49cc4b04","execution":{"iopub.status.busy":"2022-01-05T13:06:54.470445Z","iopub.execute_input":"2022-01-05T13:06:54.470887Z","iopub.status.idle":"2022-01-05T13:06:54.478265Z","shell.execute_reply.started":"2022-01-05T13:06:54.470825Z","shell.execute_reply":"2022-01-05T13:06:54.476835Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def get_event(event, filter=None):\n    zf=zipfile.ZipFile('../input/trackml-particle-identification/train_1.zip','r')\n    zipfile.ZipFile.namelist(zf)\n    hits = pd.read_csv(zf.open('train_1/%s-hits.csv'%event))\n    cells = pd.read_csv(zf.open('train_1/%s-cells.csv'%event))\n    truth = pd.read_csv(zf.open('train_1/%s-truth.csv'%event))\n    particles = pd.read_csv(zf.open('train_1/%s-particles.csv'%event))\n    return hits, cells, truth, particles","metadata":{"_uuid":"524f103d1fc55d22f79a56bd2245fbe6ae91e7a4","execution":{"iopub.status.busy":"2022-01-05T13:15:01.760684Z","iopub.execute_input":"2022-01-05T13:15:01.761223Z","iopub.status.idle":"2022-01-05T13:15:01.768445Z","shell.execute_reply.started":"2022-01-05T13:15:01.761159Z","shell.execute_reply":"2022-01-05T13:15:01.767032Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Now let us define our model architecture.","metadata":{}},{"cell_type":"code","source":"def create_model(fs = 10):\n    return nn.Sequential(\n        nn.Linear(fs, 800),\n        nn.ReLU(),\n        nn.Linear(800, 400),\n        nn.ReLU(),\n        nn.Linear(400, 400),\n        nn.ReLU(),\n        nn.Linear(400, 400),\n        nn.ReLU(),\n        nn.Linear(400, 200),\n        nn.ReLU(),\n        nn.Linear(200, 1),\n        nn.Sigmoid()\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-05T13:01:15.777201Z","iopub.execute_input":"2022-01-05T13:01:15.777605Z","iopub.status.idle":"2022-01-05T13:01:15.789838Z","shell.execute_reply.started":"2022-01-05T13:01:15.777549Z","shell.execute_reply":"2022-01-05T13:01:15.787044Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data\n+ We will use 10 events for training.\n* input: hit pair\n* output: 1 if two hits have the same particle_id, 0 otherwise.\n* feature size: 10 (5 per hit)","metadata":{"_uuid":"22ca78487ca459c532750dd133c7ed401e592e3d"}},{"cell_type":"code","source":"USE_GPU = True\n\nTRAIN_1 = True\nTRAIN_2 = True\nTRAIN = TRAIN_1 or TRAIN_2\nREDUCE_ON_PLATEAU = True\n\nLOADING_MODEL = True\nLOADING_MODEL_H = True\n\nPRE_PROCESS = True\nPRE_PROCESS_H = True\n\nSAVING = True\nLOADING_PREFIX = DATASET_PATH\nEVENT_SIZE_PATH = 'event_rows.csv'\nEVENT_SIZE_H_PATH = 'event_rows-h.csv'\n\nif USE_GPU and torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n","metadata":{"_uuid":"a286d6c7c82564b627886323f27bafad3ae47d1e","execution":{"iopub.status.busy":"2022-01-05T13:47:50.058483Z","iopub.execute_input":"2022-01-05T13:47:50.060696Z","iopub.status.idle":"2022-01-05T13:47:50.067782Z","shell.execute_reply.started":"2022-01-05T13:47:50.060626Z","shell.execute_reply":"2022-01-05T13:47:50.066741Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"def get_features(event_name):\n    hits, cells, truth, particles = get_event(event_name)\n    \n    # Filter out un-used columns early\n    hits = hits[['hit_id', 'x', 'y', 'z']]\n    truth = truth[['particle_id', 'hit_id']]    \n    \n\n    # as_index=False so the group by retain the column name\n    cell_by_hit_id = cells.groupby(['hit_id'], as_index=False)\n    cell_count = cell_by_hit_id.value.count().rename(columns={'value':'cell_count'})\n    charge_value = cell_by_hit_id.value.sum().rename(columns={'value':'charge_value'})\n    \n    # Scaling\n    hits[['x', 'y', 'z']] /= 1000\n    cell_count['cell_count'] /= 10\n    \n    truth = pd.merge(truth, cell_count, on='hit_id')\n    truth = pd.merge(truth, charge_value, on='hit_id')\n    truth = pd.merge(truth, hits, on='hit_id')\n    # The columns of truth are as follow\n    # ['particle_id', 'hit_id', 'x', 'y', 'z', 'cell_count', 'charge_value']\n    return truth\n\ndef pre_process(event_name, print_size=True):\n    features = get_features(event_name)\n    \n    columns_needed = ['x', 'y', 'z', 'cell_count', 'charge_value']\n    columns_needed_all = [c + '_x' for c in columns_needed] + [c + '_y' for c in columns_needed] + ['label']\n\n    # Get all the hits that's identified with a particle\n    true_pairs = features[features.particle_id != 0]\n    # Merge to create all hit pairs that's identified with the same particle\n    true_pairs = pd.merge(true_pairs, true_pairs, on='particle_id')\n    # Filter all the pairs that has the same hit_id\n    true_pairs = true_pairs[true_pairs.hit_id_x != true_pairs.hit_id_y]\n    # Add a new column to indicate this dataset is the true dataset\n    true_pairs['label'] = 1\n    # Filter the only columns needed\n    true_pairs = true_pairs[columns_needed_all]\n    \n    FALSE_PAIR_RATIO = 3\n    size = len(true_pairs) * FALSE_PAIR_RATIO\n    p_id = features.particle_id.values\n    # Generated random hit idx pairs\n    i = np.random.randint(len(features), size=size)\n    j = np.random.randint(len(features), size=size)\n    # Get the hit idx pair that's either assoicated with particle id 0 or different particle id\n    hit_idx = (p_id[i]==0) | (p_id[i]!=p_id[j])\n    i, j = i[hit_idx], j[hit_idx]\n    # Filter and create features with the correct order of the columns\n    features = features[columns_needed]\n    false_pairs = pd.DataFrame(\n        np.hstack((features.values[i], features.values[j], np.zeros((len(i),1)))),\n        columns=columns_needed_all)\n\n    processed = pd.concat([true_pairs, false_pairs], axis=0)\n    processed = processed.sample(frac=1).reset_index(drop=True)\n    \n    if print_size:\n        # Create a DataFrame just to pretty-print ;)\n        print(event_name)\n        print(pd\n              .DataFrame(data={\n                  'True': ['{:,}'.format(len(true_pairs))],\n                  'False': ['{:,}'.format(len(false_pairs))],\n                  'Total': ['{:,}'.format(len(processed))]\n              })\n              .to_string(index=False))\n    return processed","metadata":{"_uuid":"d400d7b6cb79d223452ac9a8af50750b9e4498e5","execution":{"iopub.status.busy":"2022-01-05T13:07:13.835359Z","iopub.execute_input":"2022-01-05T13:07:13.835675Z","iopub.status.idle":"2022-01-05T13:07:13.852271Z","shell.execute_reply.started":"2022-01-05T13:07:13.835615Z","shell.execute_reply":"2022-01-05T13:07:13.850972Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"if PRE_PROCESS:\n    event_rows = []\n    for i in tqdm_notebook(range(10, 20)):\n        event_name = 'event0000010%02d'%i\n        file_name = '%s.feather' % event_name\n        processed = pre_process(event_name)\n        event_rows.append((file_name, len(processed.index)))\n        processed.to_feather(file_name) # Save to disk\n        print('saved %s' % file_name)\n\n    pd.DataFrame(event_rows).to_csv(EVENT_SIZE_PATH, index=False)\n    print('event rows saved')\n    del processed\nelse:\n    print('load event rows')\n    event_rows = list(pd.read_csv(LOADING_PREFIX + EVENT_SIZE_PATH).itertuples(index=False, name=None))\n    event_rows = [(LOADING_PREFIX + r[0], r[1]) for r in event_rows]","metadata":{"_uuid":"21e9a3725a76c5c6fc2b1a52b1af91cfa59d1376","execution":{"iopub.status.busy":"2022-01-05T13:15:13.707386Z","iopub.execute_input":"2022-01-05T13:15:13.707719Z","iopub.status.idle":"2022-01-05T13:16:15.616360Z","shell.execute_reply.started":"2022-01-05T13:15:13.707660Z","shell.execute_reply":"2022-01-05T13:16:15.615516Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=10), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"880d37c7455e4572a4fb327fa25dcdf7"}},"metadata":{}},{"name":"stdout","text":"event000001010\nTrue      False      Total\n905,342  2,715,801  3,621,143\nsaved event000001010.feather\nevent000001011\nTrue      False      Total\n1,049,940  3,149,570  4,199,510\nsaved event000001011.feather\nevent000001012\nTrue      False      Total\n975,962  2,927,653  3,903,615\nsaved event000001012.feather\nevent000001013\nTrue      False      Total\n937,140  2,811,171  3,748,311\nsaved event000001013.feather\nevent000001014\nTrue      False      Total\n1,138,386  3,414,881  4,553,267\nsaved event000001014.feather\nevent000001015\nTrue      False      Total\n1,096,946  3,290,572  4,387,518\nsaved event000001015.feather\nevent000001016\nTrue      False      Total\n1,054,472  3,163,160  4,217,632\nsaved event000001016.feather\nevent000001017\nTrue      False      Total\n1,125,976  3,377,673  4,503,649\nsaved event000001017.feather\nevent000001018\nTrue      False      Total\n787,588  2,362,522  3,150,110\nsaved event000001018.feather\nevent000001019\nTrue      False      Total\n1,099,946  3,299,557  4,399,503\nsaved event000001019.feather\n\nevent rows saved\n","output_type":"stream"}]},{"cell_type":"code","source":"from datetime import datetime\nfrom feather import read_dataframe as feather_read\nfrom multiprocessing import current_process\nfrom threading import current_thread\nimport bisect\n\nclass FeatherCache():\n    @staticmethod\n    def cumsum(processed_rows):\n        r, s = [], 0\n        for row in processed_rows:\n            l = row[1]\n            r.append(l + s)\n            s += l\n        return r\n    \n    def __init__(self, processed_rows, cache_size=2, print_proc=False):\n        self.processed_rows = processed_rows\n        self.cumulative_sizes = self.cumsum(processed_rows)\n        self.cache_size = cache_size\n        self.print_proc = print_proc\n        \n        # warm up the loading by having two processed events loaded\n        self.cache = {}\n        for file_name, size in processed_rows[0:cache_size]:\n            self.cache[file_name] = feather_read(file_name)\n        \n        self.time_stamps = {}\n        \n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n    \n    @property\n    def LRU_filename(self):\n        least = None\n        for file_name, time_stamp in self.time_stamps.items():\n            if least is None:\n                least = (file_name, time_stamp)\n            elif time_stamp < least[1]:\n                least = (file_name, time_stamp)\n        return least[0]\n    \n    #TODO prefetch in another process when the file is loaded\n    # https://stackoverflow.com/questions/45394783/multiprocess-reading-from-file\n    def get_file_dataframe(self, file_name):\n        if file_name in self.cache:\n            # If in the cache, just get it\n            self.time_stamps[file_name] = datetime.now()\n            return self.cache[file_name]\n        else:\n            if self.print_proc:\n                process_name = current_process().name\n                thread_name = current_thread().name\n                print('reading %s from thread %s, and process %s' % (file_name, thread_name, process_name))\n            if len(self.cache) > self.cache_size:\n                key = self.LRU_filename\n                if self.print_proc:\n                    print('delete %s' % key)\n                del self.cache[key]\n                del self.time_stamps[key]\n\n            self.cache[file_name] = feather_read(file_name)\n            self.time_stamps[file_name] = datetime.now()\n            return self.cache[file_name]\n                \n    def get_map(self, indcies):\n        # Map the indices back to to file_name and its corrsponding indcies\n        file_dict = {}\n        # Optimize for single dataset\n        if (len(indcies) >= 2):\n            front_idx = bisect.bisect_right(self.cumulative_sizes, indcies[0])\n            back_idx = bisect.bisect_right(self.cumulative_sizes, indcies[-1])\n            if front_idx == back_idx:\n                file_name = self.processed_rows[front_idx][0]\n                if front_idx == 0:\n                    return {file_name : indcies}\n            \n        #else:\n        for idx in indcies:\n            dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n            if dataset_idx == 0:\n                sample_idx = idx\n            else:\n                sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n            file_name = self.processed_rows[dataset_idx][0]\n            file_dict.setdefault(file_name, []).append(sample_idx)\n        return file_dict\n    \n    def get_items(self, indcies):\n        d = self.get_map(indcies)\n        ds = None\n        for file_name in d:\n            sample_idxs = d[file_name]\n            f_ds = self.get_file_dataframe(file_name).iloc[sample_idxs]\n            if ds is None:\n                ds = f_ds\n            else:\n                ds = ds.append(f_ds, ignore_index=True)\n        return ds\n            \nclass FeatherDataset(Dataset):\n    def __init__(self, feather_cache, to_items_fn=None):\n        self.cache = feather_cache\n        self.to_items_fn = to_items_fn\n        \n    def __len__(self):\n        return len(self.cache)\n\n    def __getitem__(self, idx):\n        return idx;\n    \n    def get_items(self, indcies):\n        ds = self.cache.get_items(indcies)\n        rows = torch.as_tensor(ds.values)\n        return rows if self.to_items_fn is None else self.to_items_fn(rows)\n    \n    @property\n    def collate_fn(self):\n        return self.get_items\n\nclass DataframeDataset(Dataset):\n    def __init__(self, dataframe, opti_seq=False, to_items_fn=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataframe = dataframe\n        self.opti_seq = opti_seq\n        self.to_items_fn = to_items_fn\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        return idx\n    \n    def get_items(self, indcies):\n        if self.opti_seq and len(indcies) >= 2:\n            rows = torch.as_tensor(self.dataframe.values[indcies[0]:indcies[-1]+1])\n        else:\n            rows = torch.as_tensor(self.dataframe.iloc[indcies].values)\n        return rows if self.to_items_fn is None else self.to_items_fn(rows)\n    \n    @property\n    def collate_fn(self):\n        return self.get_items","metadata":{"_uuid":"fbd0b08a9020aa3eada8a1fa33b1b825702aeee1","execution":{"iopub.status.busy":"2022-01-05T13:16:28.411375Z","iopub.execute_input":"2022-01-05T13:16:28.411701Z","iopub.status.idle":"2022-01-05T13:16:28.441092Z","shell.execute_reply.started":"2022-01-05T13:16:28.411642Z","shell.execute_reply":"2022-01-05T13:16:28.440012Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class SequentialRangeSampler(Sampler):\n    def __init__(self, data_source, num_samples=None):\n        self.data_source = data_source\n        self.num_samples = range(len(self.data_source)) if num_samples is None else num_samples\n\n    def __iter__(self):\n        return iter(self.num_samples)\n\n    def __len__(self):\n        return len(self.data_source)","metadata":{"_uuid":"506fe3dd02bf1949dc0111d7530df826b1c870fb","execution":{"iopub.status.busy":"2022-01-05T13:16:34.653467Z","iopub.execute_input":"2022-01-05T13:16:34.653812Z","iopub.status.idle":"2022-01-05T13:16:34.659812Z","shell.execute_reply.started":"2022-01-05T13:16:34.653752Z","shell.execute_reply":"2022-01-05T13:16:34.658676Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def create_loaders(dataset, batch_size, validation_split):\n    dataset_size = len(dataset)\n    num_val = int(validation_split * dataset_size)\n    num_train = dataset_size - num_val\n\n    loader_train = DataLoader(dataset, batch_size=batch_size, num_workers=0, pin_memory=True,\n                              sampler=SequentialRangeSampler(range(num_train)),\n                              collate_fn=dataset.collate_fn)\n    loader_val = DataLoader(dataset, batch_size=batch_size, num_workers=0, pin_memory=True,\n                            sampler=SequentialRangeSampler(range(num_train, dataset_size)),\n                            collate_fn=dataset.collate_fn)\n    return loader_train, loader_val","metadata":{"_uuid":"17badacb2e7c17d15ae6ba368ee40609cf0dfe5d","execution":{"iopub.status.busy":"2022-01-05T13:16:36.902150Z","iopub.execute_input":"2022-01-05T13:16:36.902440Z","iopub.status.idle":"2022-01-05T13:16:36.910843Z","shell.execute_reply.started":"2022-01-05T13:16:36.902383Z","shell.execute_reply":"2022-01-05T13:16:36.909327Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"if TRAIN_1:\n    batch_size = 8000\n    validation_split = .05 # 5%\n    cache = FeatherCache(event_rows)\n    dataset = FeatherDataset(cache, lambda rows : (rows[:, :-1], rows[:, -1].view(-1, 1)))\n    loader_train, loader_val = create_loaders(dataset, batch_size, validation_split)","metadata":{"_uuid":"ac809d0ae97a75d8e1d52d6856b5905950b1afd9","execution":{"iopub.status.busy":"2022-01-05T13:16:49.991821Z","iopub.execute_input":"2022-01-05T13:16:49.992168Z","iopub.status.idle":"2022-01-05T13:16:50.481826Z","shell.execute_reply.started":"2022-01-05T13:16:49.992108Z","shell.execute_reply":"2022-01-05T13:16:50.480710Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Training the model\nWe have already defined model architecture, dataloader. Now let us define the accuracy and train 1 epoch function.","metadata":{"_uuid":"9f38fbd32829f9ca2a18f8b00bf3db26f57f4297"}},{"cell_type":"code","source":"def check_accuracy(loader, model, thr=0.5):\n    num_correct = 0\n    num_samples = 0\n    model.eval()  # set model to evaluation mode\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n            y = y.view(-1).to(device=device, dtype=torch.uint8)\n            scores = model(x)\n            scores = (scores > thr).view(-1)\n            num_correct += (scores == y).sum()\n            num_samples += scores.size(0)\n        acc = float(num_correct) / num_samples\n        return (num_correct, num_samples, acc)","metadata":{"_uuid":"81384ee15d418ecb454d4b78b8e1ebd351f78ed3","execution":{"iopub.status.busy":"2022-01-05T13:16:58.835542Z","iopub.execute_input":"2022-01-05T13:16:58.835948Z","iopub.status.idle":"2022-01-05T13:16:58.849088Z","shell.execute_reply.started":"2022-01-05T13:16:58.835888Z","shell.execute_reply":"2022-01-05T13:16:58.847788Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def train_model(model, optimizer, criterion, loader_train, loader_val, epochs=1, reduce_on_plateau=False, epoch_callback=None):\n    with elapsed_timer() as elapser:\n        model = model.to(device=device)  # move the model parameters to CPU/GPU\n        total_second = 0\n        if reduce_on_plateau:\n            scheduler = ReduceLROnPlateau(optimizer, 'max', patience=5, threshold=1e-3, verbose=True)\n        for e in tqdm_notebook(range(epochs)):\n            begin_epoch = elapser()\n            for t, (x, y) in enumerate(tqdm_notebook(loader_train, desc='Epoch %d' % e, leave=False)):\n                model.train()  # put model to training mode\n                x = x.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n                y = y.view(-1, 1).to(device=device, dtype=torch.float) # BCELoss only support float as y\n                optimizer.zero_grad()\n                scores = model(x)\n                loss = criterion(scores, y)\n                loss.backward()\n                optimizer.step()\n\n            num_correct, num_samples, acc = check_accuracy(loader_val, model)\n            end_epoch = elapser()\n            print('%.2fs - Epoch %d, Iteration %d, loss = %.4f, %d / %d correct (%.2f %%)' % (end_epoch - begin_epoch, e, t, loss.item(), num_correct, num_samples, acc * 100))\n            if epoch_callback is not None:\n                epoch_callback(num_correct, num_samples, acc, loss)\n            if reduce_on_plateau:\n                scheduler.step(acc)\n    print('Total time: %.2fs' % elapser())","metadata":{"_uuid":"287159410b61fe0d91390e5450bf628beda1641c","execution":{"iopub.status.busy":"2022-01-05T13:18:19.493594Z","iopub.execute_input":"2022-01-05T13:18:19.493965Z","iopub.status.idle":"2022-01-05T13:18:19.504427Z","shell.execute_reply.started":"2022-01-05T13:18:19.493904Z","shell.execute_reply":"2022-01-05T13:18:19.503025Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model_torch = create_model().to(device)","metadata":{"_uuid":"197bbb13286aad38526e66ae7738518f042d05cf","execution":{"iopub.status.busy":"2022-01-05T13:24:32.515767Z","iopub.execute_input":"2022-01-05T13:24:32.516132Z","iopub.status.idle":"2022-01-05T13:24:32.534961Z","shell.execute_reply.started":"2022-01-05T13:24:32.516056Z","shell.execute_reply":"2022-01-05T13:24:32.533995Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"if TRAIN_1 and REDUCE_ON_PLATEAU:\n    lr = -3\n    optimizer = optim.Adam(model_torch.parameters(), lr=10**lr)\n    criterion = nn.BCELoss()\n    train_model(model_torch, optimizer, criterion, loader_train, loader_val, epochs=4, reduce_on_plateau=True)","metadata":{"_uuid":"6afc7c09d7fb68853cc616e4b4fa32007f6fb383","execution":{"iopub.status.busy":"2022-01-05T13:24:34.285182Z","iopub.execute_input":"2022-01-05T13:24:34.285495Z","iopub.status.idle":"2022-01-05T13:32:40.902075Z","shell.execute_reply.started":"2022-01-05T13:24:34.285421Z","shell.execute_reply":"2022-01-05T13:32:40.901003Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=4), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a0a8561ee9433485d301a82de864e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 0', max=4832, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"126.26s - Epoch 0, Iteration 4831, loss = 0.0479, 2002823 / 2034212 correct (98.46 %)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 1', max=4832, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"125.77s - Epoch 1, Iteration 4831, loss = 0.0441, 2007592 / 2034212 correct (98.69 %)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 2', max=4832, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"118.16s - Epoch 2, Iteration 4831, loss = 0.0318, 2008918 / 2034212 correct (98.76 %)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 3', max=4832, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"116.36s - Epoch 3, Iteration 4831, loss = 0.0350, 2011454 / 2034212 correct (98.88 %)\nTotal time: 486.61s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let me save our model first.","metadata":{}},{"cell_type":"code","source":"if TRAIN_1 and SAVING:\n    print('saving model')\n    torch.save(model_torch.state_dict(), 'torch_model.pt')","metadata":{"_uuid":"ae9036c2ec28f16aa3027e12dae5b14099d860cc","execution":{"iopub.status.busy":"2022-01-05T13:34:10.107298Z","iopub.execute_input":"2022-01-05T13:34:10.107675Z","iopub.status.idle":"2022-01-05T13:34:10.122659Z","shell.execute_reply.started":"2022-01-05T13:34:10.107573Z","shell.execute_reply":"2022-01-05T13:34:10.121218Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"saving model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We saved our model, now if we want to load it again, we'll just execute\\\n`the_model = model_torch()`\\\n`the_model.load_state_dict(torch.load('torch_model.pt'))`","metadata":{}},{"cell_type":"markdown","source":"## Hard Negative Mining","metadata":{"_uuid":"185872dc6a27b54f7b810c66a5076796c8c8ec01"}},{"cell_type":"markdown","source":"Our model landed an accuracy of 98.88% which is quite impressive but we need to decrease our false positives even more and ultimately boost model accuracy therefore I will now utilise Hard Negative Mining, ie. we will simply create more negative samples and train our model to see the results.","metadata":{}},{"cell_type":"code","source":"def predict(dataframe, model, batch_size=8000, num_worker=0):\n    rows = torch.as_tensor(dataframe.values)\n    num_elements = len(rows)\n    num_batches = -(-num_elements // batch_size) # Round up\n    model.eval()  # set model to evaluation mode\n    scores = torch.zeros(num_elements, dtype=torch.float)\n    with torch.no_grad():\n        for i in range(num_batches):\n            start = i * batch_size\n            end = num_elements if i == num_batches - 1 else start + batch_size\n            x_batch = rows[start:end]\n            x_batch = x_batch.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n            scores[start:end] = model(x_batch).view(-1)\n\n    return scores\n\ndef predict_true(dataframe, model, batch_size=8000, thr=0.5):\n    scores = predict(dataframe, model, batch_size)\n    indices = (scores > thr).nonzero()[:, 0]\n    return dataframe.iloc[indices]","metadata":{"_uuid":"73f15df6a16f5837fbcdf0d49fce6bfabb4db2d6","execution":{"iopub.status.busy":"2022-01-05T13:37:20.215382Z","iopub.execute_input":"2022-01-05T13:37:20.215700Z","iopub.status.idle":"2022-01-05T13:37:20.223645Z","shell.execute_reply.started":"2022-01-05T13:37:20.215637Z","shell.execute_reply":"2022-01-05T13:37:20.222494Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def negative_mine(model, event_name, smaple_size=30000000, print_size=True):\n    with elapsed_timer() as elapser: \n        hits, cells, truth, particles = get_event(event_name)\n\n        # Filter out un-used columns early\n        hits = hits[['hit_id', 'x', 'y', 'z']]\n        truth = truth[['particle_id', 'hit_id']]    \n\n\n        # as_index=False so the group by retain the column name\n        cell_by_hit_id = cells.groupby(['hit_id'], as_index=False)\n        cell_count = cell_by_hit_id.value.count().rename(columns={'value':'cell_count'})\n        charge_value = cell_by_hit_id.value.sum().rename(columns={'value':'charge_value'})\n\n        # Scaling\n        hits[['x', 'y', 'z']] /= 1000\n        cell_count['cell_count'] /= 10\n\n        features = pd.merge(truth, cell_count, on='hit_id')\n        features = pd.merge(features, charge_value, on='hit_id')\n        features = pd.merge(features, hits, on='hit_id')\n        # The columns of truth are as follow\n        # ['particle_id', 'hit_id', 'x', 'y', 'z', 'cell_count', 'charge_value']\n\n        columns_needed = ['x', 'y', 'z', 'cell_count', 'charge_value']\n        columns_needed_all = [c + '_x' for c in columns_needed] + [c + '_y' for c in columns_needed]\n\n        p_id = features.particle_id.values\n        # Generated random hit idx pairs\n        i = np.random.randint(len(features), size=smaple_size)\n        j = np.random.randint(len(features), size=smaple_size)\n        # Get the hit idx pair that's either assoicated with particle id 0 or different particle id\n        hit_idx = (p_id[i]==0) | (p_id[i]!=p_id[j])\n        i, j = i[hit_idx], j[hit_idx]\n        # Filter and create features with the correct order of the columns\n        features = features[columns_needed]\n        false_pairs = pd.DataFrame(\n            np.hstack((features.values[i], features.values[j])),\n            columns=columns_needed_all)\n\n        before_size = len(false_pairs)\n        false_pairs = predict_true(false_pairs, model_torch).reset_index(drop=True)\n        false_pairs['label'] = 0\n        after_size = len(false_pairs)\n        if print_size:\n            print(event_name)\n            print('%.2fs - Before: %s, After: %s, Percent Pass: %d%%' % (elapser(), '{:,}'.format(before_size), '{:,}'.format(after_size), after_size/before_size*100))\n        return false_pairs","metadata":{"_uuid":"4dddcfc030bfe654e8032e478c29b4e9b74df9fd","execution":{"iopub.status.busy":"2022-01-05T13:37:21.962443Z","iopub.execute_input":"2022-01-05T13:37:21.962757Z","iopub.status.idle":"2022-01-05T13:37:21.976380Z","shell.execute_reply.started":"2022-01-05T13:37:21.962696Z","shell.execute_reply":"2022-01-05T13:37:21.974997Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def preprocess_h():\n    event_rows_h = []\n    for idx, i in enumerate(tqdm_notebook(range(10,20))):\n        event_name = 'event0000010%02d' % i\n        file_name = '%s.feather' % event_name\n        processed_negative = negative_mine(model_torch, event_name)\n        with elapsed_timer() as elapser:\n            processed = feather_read(event_rows[idx][0]) # read the path from event_rows loaded\n            processed = processed.append(processed_negative, ignore_index=True)\n            processed = processed.sample(frac=1).reset_index(drop=True)\n            print('Read, append and re-sample: %.2fs' % elapser())\n        event_rows_h.append((file_name, len(processed.index)))\n        processed.to_feather(file_name) # Save to disk\n        print('saved %s' % file_name)\n\n    pd.DataFrame(event_rows_h).to_csv(EVENT_SIZE_H_PATH, index=False)\n    print('event rows h saved')\n    return event_rows_h\n\n# if you skip step2, you still need to run step1 to get training data.\nif LOADING_MODEL:\n    print('load model')\n    model_torch.load_state_dict(torch.load('torch_model.pt'))\n\n# Preprocess\nif PRE_PROCESS_H:\n    event_rows_h = preprocess_h()\nelse:\n        print('load event rows hard')\n        event_rows_h = list(pd.read_csv(LOADING_PREFIX + EVENT_SIZE_H_PATH).itertuples(index=False, name=None))\n        event_rows_h = [(LOADING_PREFIX + r[0], r[1]) for r in event_rows_h]","metadata":{"_uuid":"2d4d11c929200b31498b005b41a87eec6adf477c","execution":{"iopub.status.busy":"2022-01-05T13:42:17.391782Z","iopub.execute_input":"2022-01-05T13:42:17.392230Z","iopub.status.idle":"2022-01-05T13:46:34.280482Z","shell.execute_reply.started":"2022-01-05T13:42:17.392069Z","shell.execute_reply":"2022-01-05T13:46:34.279633Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"load model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=10), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da42e4e8f2e04d3690a224964329985e"}},"metadata":{}},{"name":"stdout","text":"event000001010\n21.89s - Before: 29,997,177, After: 240,187, Percent Pass: 0%\nRead, append and re-sample: 2.12s\nsaved event000001010.feather\nevent000001011\n21.70s - Before: 29,997,486, After: 235,823, Percent Pass: 0%\nRead, append and re-sample: 2.68s\nsaved event000001011.feather\nevent000001012\n22.27s - Before: 29,997,333, After: 230,943, Percent Pass: 0%\nRead, append and re-sample: 2.47s\nsaved event000001012.feather\nevent000001013\n22.28s - Before: 29,997,320, After: 232,271, Percent Pass: 0%\nRead, append and re-sample: 2.61s\nsaved event000001013.feather\nevent000001014\n22.12s - Before: 29,997,619, After: 235,950, Percent Pass: 0%\nRead, append and re-sample: 2.92s\nsaved event000001014.feather\nevent000001015\n21.91s - Before: 29,997,637, After: 237,265, Percent Pass: 0%\nRead, append and re-sample: 2.89s\nsaved event000001015.feather\nevent000001016\n21.73s - Before: 29,997,525, After: 232,235, Percent Pass: 0%\nRead, append and re-sample: 2.66s\nsaved event000001016.feather\nevent000001017\n22.36s - Before: 29,997,537, After: 231,383, Percent Pass: 0%\nRead, append and re-sample: 2.81s\nsaved event000001017.feather\nevent000001018\n22.02s - Before: 29,996,977, After: 238,165, Percent Pass: 0%\nRead, append and re-sample: 2.17s\nsaved event000001018.feather\nevent000001019\n22.03s - Before: 29,997,638, After: 235,362, Percent Pass: 0%\nRead, append and re-sample: 2.57s\nsaved event000001019.feather\nevent rows h saved\n","output_type":"stream"}]},{"cell_type":"code","source":"if TRAIN_2:\n    batch_size = 8000\n    validation_split = .05 # 5%\n    cache = FeatherCache(event_rows_h[::-1]) # invert to switch it up?\n    dataset = FeatherDataset(cache, lambda rows : (rows[:, :-1], rows[:, -1].view(-1, 1)))\n    loader_train, loader_val = create_loaders(dataset, batch_size, validation_split)","metadata":{"_uuid":"7d82dea265c632c7f7b734449b1a2a99ea93028b","scrolled":true,"execution":{"iopub.status.busy":"2022-01-05T13:48:01.083514Z","iopub.execute_input":"2022-01-05T13:48:01.083861Z","iopub.status.idle":"2022-01-05T13:48:01.585729Z","shell.execute_reply.started":"2022-01-05T13:48:01.083792Z","shell.execute_reply":"2022-01-05T13:48:01.584905Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"if TRAIN_2 and REDUCE_ON_PLATEAU:\n    lr = -3\n    optimizer = optim.Adam(model_torch.parameters(), lr=10**lr)\n    criterion = nn.BCELoss()\n    train_model(model_torch, optimizer, criterion, loader_train, loader_val, epochs=5, reduce_on_plateau=True)","metadata":{"_uuid":"26c88b7ce0fa96f7ff7d97696561cc2eadf67cf7","execution":{"iopub.status.busy":"2022-01-05T13:48:57.586509Z","iopub.execute_input":"2022-01-05T13:48:57.586815Z","iopub.status.idle":"2022-01-05T13:59:10.360107Z","shell.execute_reply.started":"2022-01-05T13:48:57.586759Z","shell.execute_reply":"2022-01-05T13:59:10.359018Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=5), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b4cc5debc64099a67ac7205b706b96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 0', max=5111, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"124.55s - Epoch 0, Iteration 5110, loss = 0.0652, 2089755 / 2151692 correct (97.12 %)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 1', max=5111, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"122.31s - Epoch 1, Iteration 5110, loss = 0.0607, 2093197 / 2151692 correct (97.28 %)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 2', max=5111, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"121.31s - Epoch 2, Iteration 5110, loss = 0.0569, 2094933 / 2151692 correct (97.36 %)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 3', max=5111, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"122.89s - Epoch 3, Iteration 5110, loss = 0.0539, 2095947 / 2151692 correct (97.41 %)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch 4', max=5111, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"121.67s - Epoch 4, Iteration 5110, loss = 0.0513, 2097183 / 2151692 correct (97.47 %)\nTotal time: 612.77s\n","output_type":"stream"}]},{"cell_type":"code","source":"if TRAIN_2 and SAVING:\n    torch.save(model_torch.state_dict(), 'torch_model_h.pt')\nif TRAIN_2:\n    del loader_train\n    del loader_val","metadata":{"_uuid":"b4f40ace4294695b6efeee9e8492d81239b1c26a","execution":{"iopub.status.busy":"2022-01-05T13:59:25.914814Z","iopub.execute_input":"2022-01-05T13:59:25.915195Z","iopub.status.idle":"2022-01-05T13:59:25.928352Z","shell.execute_reply.started":"2022-01-05T13:59:25.915105Z","shell.execute_reply":"2022-01-05T13:59:25.927160Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Wow, we are done the training!","metadata":{}}]}